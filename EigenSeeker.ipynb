{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "\n",
    "rd.seed(6666) # devil with extra digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Idea: Find the (Non-Complex) Eigenvalue of a Matrix\n",
    "\n",
    "It's from scratch-ish with some use of `numpy` because I don't want to write a hundred for-loops.\n",
    "\n",
    "## References\n",
    "\n",
    "* Lots of videos by **Mu Prime Math** ([used playlist](https://www.youtube.com/playlist?list=PLug5ZIRrShJHNCfEiX6l5CKbljWayGEcs))\n",
    "* [Also this video](https://www.youtube.com/watch?v=LHlg_lfihiA)\n",
    "* [Also this link](https://math.nyu.edu/~stadler/num1/material/num1_eigenvalues.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Method\n",
    "\n",
    "An iterative algorithm that finds the eigenvalue with the highest magnitude.\n",
    "\n",
    "### Update Rule\n",
    "\n",
    "$$ b_{k+1} = \\frac{Ab_k}{||Ab_k||} $$\n",
    "\n",
    "such that\n",
    "\n",
    "* $A$ is an $n\\times n$ matrix\n",
    "* $b_k$ is an $n$-element vector\n",
    "* $||Ab_k||$ is the element with the highest magnitude of $Ab_k$\n",
    "* $b_0$ is non-zero\n",
    "* Stop when $b_k = b_{k+1}$ (once the scaled $b_{k+1}$ is the same as $b_k$)\n",
    "\n",
    "Once convergence is reached, we will end up with an eigenvector of $A$ that corresponds to the \n",
    "eigenvalue with the biggest magnitude. To find the eigenvalue from this, we can scale down the\n",
    "eigenvector $\\vec{v}$ so the largest element is 1, multiply $\\vec{v}$ by $A$,\n",
    "then the eigenvalue is the element that has the highest magnitude.\n",
    "\n",
    "Note: The code doesn't do this exactly but the code looks cleaner this way.\n",
    "\n",
    "### Proof\n",
    "\n",
    "Let's say we have matrix $A$ with eigenvectors $\\vec{v_1}, \\vec{v_2}, \\ldots, \\vec{v_n}$ with \n",
    "corresponding eigenvalues $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$.\n",
    "\n",
    "Let $\\vec{x}$ be written as a linear combination of the eigenvectors of $A$.\n",
    "\n",
    "$$ \\vec{x} = c_1\\vec{v_1} + c_2\\vec{v_2} + \\ldots + c_n\\vec{v_n} $$\n",
    "\n",
    "We can then multiply $A$ to both sides, giving\n",
    "\n",
    "$$ A\\vec{x} = Ac_1\\vec{v_1} + Ac_2\\vec{v_2} + \\ldots + Ac_n\\vec{v_n} $$\n",
    "\n",
    "If we keep multiplying $A$ to both sides (let's say $k$ times), we will end up with\n",
    "\n",
    "$$ A^k\\vec{x} = A^kc_1\\vec{v_1} + A^kc_2\\vec{v_2} + \\ldots + A^kc_n\\vec{v_n} $$\n",
    "\n",
    "Because $\\vec{v_1}, \\vec{v_2}, \\ldots, \\vec{v_n}$ are eigenvectors of $A$, we know that $A\\vec{v_i} = \\lambda_i\\vec{v_i} $.\n",
    "We also know that if $\\lambda$ is an eigenvalue of $A$, $\\lambda^k$ would be the eigenvalue of $A^k$.\n",
    "Thus, we get\n",
    "\n",
    "$$ A^k\\vec{x} = c_1\\lambda_1^k\\vec{v_1} + c_2\\lambda_2^k\\vec{v_2} + \\ldots + c_n\\lambda_n^k\\vec{v_n} $$\n",
    "\n",
    "Let $\\lambda_1$ be the eigenvalue of $A$ with the highest magnitude. We can factor it out to get\n",
    "\n",
    "$$ A^k\\vec{x} = \\lambda_1^k\\left(c_1\\vec{v_1} + \\frac{c_2\\lambda_2^k\\vec{v_2}}{\\lambda_1^k} + \\ldots + \\frac{c_n\\lambda_n^k\\vec{v_n}}{\\lambda_1^k}\\right) $$\n",
    "\n",
    "Because $|\\lambda_1| > |\\lambda_2|, \\ldots, |\\lambda_n|$,\n",
    "\n",
    "$$\\lim_{k\\to\\infty} \\frac{\\lambda_i^k}{\\lambda_1^k} = 0~\\forall i\\neq1 $$\n",
    "\n",
    "Therefore, we will end up with this when $k$ is very big\n",
    "\n",
    "$$ A^k\\vec{x} =  c_1 \\lambda_1^k \\vec{v_1} $$\n",
    "\n",
    "This means that we can just choose an arbitrary $\\vec{x}$ to find the eigenvalue with the\n",
    "highest magnitude and its corresponding eigenvector as long as we multiply $A$ enough times.\n",
    "\n",
    "We will end up with a scaled version of an eigenvector, denoted as $c\\vec{v}$ for some constant $c$.\n",
    "\n",
    "For the finding eigenvalue part, this is because $A\\vec{v} = \\lambda{v}$. Therefore, $\\lambda\\times1 = \\lambda$\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_eig(A:np.ndarray) -> Tuple[float, np.ndarray]:\n",
    "\tsize, _ = A.shape\n",
    "\tA = A.astype(float)\n",
    "\tvec = np.ones(size).astype(float)\n",
    "\tpvec = np.zeros(size).astype(float)\n",
    "\n",
    "\twhile (vec != pvec).all():\n",
    "\t\tpvec = vec\n",
    "\t\timax = np.argmax(np.abs(vec))\n",
    "\t\tvec = np.matmul(A, vec) / vec[imax]\n",
    "\n",
    "\treturn vec[imax], vec/vec[imax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0000000000000018 [ 1.          0.42857143 -0.71428571]\n"
     ]
    }
   ],
   "source": [
    "test_matrix = np.array([[5., 8., 16.],\n",
    "\t\t\t\t\t\t[4., 1., 8.],\n",
    "\t\t\t\t\t\t[-4., -4., -11.]])\n",
    "max_eival, max_eivec = power_eig(test_matrix)\n",
    "print(max_eival, max_eivec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvector:\t [ 1.          0.42857143 -0.71428571]\n",
      "A*eivec:\t [-3.         -1.28571429  2.14285714]\n",
      "eival*eivec:\t [-3.         -1.28571429  2.14285714]\n"
     ]
    }
   ],
   "source": [
    "print(\"eigenvector:\\t\", max_eivec)\n",
    "print(\"A*eivec:\\t\", np.matmul(test_matrix, max_eivec))\n",
    "print(\"eival*eivec:\\t\", max_eival * max_eivec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifted Inverse Power Method\n",
    "\n",
    "An iterative algorithm for finding eigenvalues that isn't the biggest one.\n",
    "\n",
    "The problem with the power method is that it only\n",
    "works for finding the eigenvalue and eigenvector for the eigenvalue that has the highest magnitude\n",
    "due to it *dominating* the other terms as shown above.\n",
    "\n",
    "Thus, we have to modify the power method so that it works with finding the other eigenvalues of the\n",
    "using the shifted inverse power method.\n",
    "\n",
    "### Update Rule\n",
    "\n",
    "$$ b_{k+1} = \\frac{(A - \\alpha I)^{-1}b_k}{||(A - \\alpha I)^{-1}b_k||} $$\n",
    "\n",
    "such that:\n",
    "* $\\alpha$ is the initial guess for the eigenvalue; \"*the shift*\"\n",
    "* The rest are similar to the regular power method\n",
    "* Gives us the eigenvalue that is closest to $\\alpha$ and its corresponding eigenvector\n",
    "\n",
    "Once convergence is reached, we'll end up with eigenvector $\\vec{v}$ that is an eigenvector of $A$\n",
    "which happens to correspond to $(\\lambda - \\alpha)^{-1}$.\n",
    "\n",
    "Let $\\omega = (\\lambda - \\alpha)^{-1} = \\frac{1}{\\lambda - \\alpha}$,\n",
    "\n",
    "$$ \\lambda = \\frac{1}{\\omega} + \\alpha $$\n",
    "\n",
    "where $\\lambda$ is the eigenvalue that is closest to $\\alpha$.\n",
    "\n",
    "### Proof\n",
    "\n",
    "Recall the definition of eigenvalue and eigenvector of matrix $A$:\n",
    "\n",
    "$$ A\\vec{v} = \\lambda\\vec{v} $$\n",
    "\n",
    "The following parts will seem like it came out of thin air. That's because it is. Bare with me.\n",
    "\n",
    "We can subtract $\\alpha\\vec{v}$ from both sides\n",
    "in the appropriate formats\n",
    "\n",
    "$$ A\\vec{v} - \\alpha I\\vec{v} = \\lambda\\vec{v} - \\alpha\\vec{v} $$\n",
    "\n",
    "We can then simplify it down to \n",
    "\n",
    "$$ (A - \\alpha I)\\vec{v} = (\\lambda - \\alpha)\\vec{v} $$\n",
    "\n",
    "Then, we can multiply the inverse of $A - \\alpha I$ to both sides then divide both sides by\n",
    "$\\lambda - \\alpha$ to get\n",
    "\n",
    "$$ (A - \\alpha I)^{-1}\\vec{v} = \\frac{1}{\\lambda - \\alpha}\\vec{v} \\equiv (\\lambda - \\alpha)^{-1}\\vec{v} $$\n",
    "\n",
    "This is basically the definition of eigenvalue-vector as well but for $(A - \\alpha I)^{-1}$.\n",
    "However, $\\vec{v}$ will still be the eigenvector of $A$ as well because this whole thing was based\n",
    "on the \"typical\" definition of the matrix. We just did a bit of manipulation on them.\n",
    "\n",
    "Now, we can use the power method on this *fancy new matrix* and its *fancy new value*.\n",
    "\n",
    "Let $$ B = (A - \\alpha I)^{-1} , \\omega = (\\lambda - \\alpha)^{-1} $$\n",
    "where $\\omega$ is the eigenvalue of $B$ with the highest magnitude.\n",
    "\n",
    "Let us write $\\vec{x}$ as a linear combination of eigenvectors $\\vec{v}_1, \\ldots, \\vec{v}_n$ of $B$.\n",
    "\n",
    "$$ \\vec{x} = c_1\\vec{v_1} + \\ldots + c_n\\vec{v_n} $$\n",
    "\n",
    "Since this is similar to the regular power method, let us speed through this. We can multiply $B$\n",
    "to both sides $k$ times and we'll eventually end up with this when $k$ is very big\n",
    "\n",
    "$$ B^k\\vec{x} = c\\omega^k\\vec{v} $$\n",
    "\n",
    "Finally, we can do algebra to convert it back to fit $A$.\n",
    "\n",
    "As for why $\\lambda$ is the eigenvalue closest to $\\alpha$, here's the explanation.\n",
    "\n",
    "For the power method, the reason that we are able to find the eigenvalue $\\lambda$ with the largest magnitude\n",
    "from it is because the value of $\\lambda$ would dominate the rest with enough multiplications.\n",
    "\n",
    "When we change from $\\lambda$ by itself to $\\frac{1}{\\lambda - \\alpha}$, this means that the closer\n",
    "$\\lambda$ is to $\\alpha$, the bigger the term will be. Thus, it will become the dominating term instead.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_eig(A:np.ndarray, alpha:float):\n",
    "\tsize, _ = A.shape\n",
    "\tvec = np.ones(size).astype(float)\n",
    "\tpvec = np.zeros(size).astype(float)\n",
    "\tA = A.astype(float)\n",
    "\tinv = np.linalg.inv(A - alpha*np.identity(size))\n",
    "\n",
    "\twhile (vec != pvec).all():\n",
    "\t\tpvec = vec\n",
    "\t\timax = np.argmax(np.abs(vec))\n",
    "\t\tvec = np.matmul(inv, vec) / vec[imax]\n",
    "\n",
    "\treturn np.round(1/vec[imax]+alpha, 6), vec/vec[imax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 [ 1.   0.5 -0.5]\n"
     ]
    }
   ],
   "source": [
    "eival, eivec = shifted_eig(test_matrix, 1.1)\n",
    "print(eival, eivec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvector:\t [ 1.   0.5 -0.5]\n",
      "A*eivec:\t [ 1.   0.5 -0.5]\n",
      "eival*eivec:\t [ 1.   0.5 -0.5]\n"
     ]
    }
   ],
   "source": [
    "print(\"eigenvector:\\t\", eivec)\n",
    "print(\"A*eivec:\\t\", np.matmul(test_matrix, eivec))\n",
    "print(\"eival*eivec:\\t\", eival * eivec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gershgorin Circle Theorem\n",
    "\n",
    "The theorem states that the eigenvalues are going to lie somewhere within the circle around the\n",
    "diagonal entries where the radius is the absolute sum of the remaining entries of the corresponding row.\n",
    "\n",
    "### Proof\n",
    "\n",
    "***The proof is by magic.***\n",
    "\n",
    "I'm joking. *But if you close your eyes...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us start from the definition of the eigenvalue $\\lambda$ and eigenvector $\\vec{v}$:\n",
    "$A\\vec{v} = \\lambda\\vec{v}$. Let $i$ be the index of the entry in $\\vec{v}$ with the largest magnitude.\n",
    "\n",
    "We can write this expression as a summation where we'll get\n",
    "\n",
    "$$ \\sum_{c=1}^n A_{ic}\\vec{v}_c = \\lambda\\vec{v}_i $$\n",
    "where $A_{ic}$ is the entry on row $i$ and column $c$, and $\\vec{v}_i$ is the $i^{th}$ entry of $\\vec{v}$.\n",
    "\n",
    "We can then further split this up into more parts\n",
    "\n",
    "$$ A_{ii}\\vec{v}_i + \\sum_{c\\neq i} A_{ic}\\vec{v}_c = \\lambda\\vec{v}_i $$\n",
    "\n",
    "We can then manipulate this a bit to get\n",
    "\n",
    "$$ \\sum_{c\\neq i} A_{ic}\\vec{v}_c = \\lambda\\vec{v}_i - A_{ii}\\vec{v}_i $$\n",
    "$$ \\sum_{c\\neq i} A_{ic}\\vec{v}_c = (\\lambda - A_{ii})\\vec{v}_i $$\n",
    "$$ \\sum_{c\\neq i} A_{ic}\\frac{\\vec{v}_c}{\\vec{v}_i} = \\lambda - A_{ii} $$\n",
    "\n",
    "We can take the absolute of both sides and get an inequality from the property of absolutes.\n",
    "\n",
    "$$ |\\lambda - A_{ii}| = \\sum_{c\\neq i} |A_{ic}||\\frac{\\vec{v}_c}{\\vec{v}_i}| $$\n",
    "\n",
    "Since $\\vec{v}_i$ is the entry with the largest magnitude of $\\vec{v}$,\n",
    "$$\\frac{\\vec{v}_c}{\\vec{v}_i} \\leq 1 \\forall c$$\n",
    "\n",
    "As a result, we are going to ignore the term wholely because this is already an inequality.\n",
    "Finally, we will get \n",
    "$$ |\\lambda - A_{ii}| = \\sum_{c\\neq i} |A_{ic}| $$\n",
    "which can be translated into the distance between $\\lambda$ and the diagonal entry of $A$ is\n",
    "going to be less than or equal to the sum of the non-diagonal entries of the corresponding row.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abssum_except(lst, idx):\n",
    "\treturn np.sum([ abs(l) for i, l in enumerate(lst) if i != idx ])\n",
    "\n",
    "def gersh_circle(A:np.ndarray):\n",
    "\tn, _ = A.shape\n",
    "\n",
    "\talphas = []\n",
    "\tfor i in range(n):\n",
    "\t\tdiag_entry = A[i, i]\n",
    "\t\trad = abssum_except(A[i], i)\n",
    "\t\talpha = rd.random()*2*rad + diag_entry - rad\n",
    "\t\talphas.append(alpha-rd.random())\n",
    "\t\n",
    "\treturn alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.275986447836338, 9.65154693212159, -10.79076338617846]"
      ]
     },
     "execution_count": 822,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gersh_circle(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.+0.00000000e+00j -3.+2.71947991e-15j -3.-2.71947991e-15j]\n",
      "(1.0, array([ 1. ,  0.5, -0.5]))\n",
      "(-3.0, array([ 1.        ,  0.42857143, -0.71428571]))\n",
      "(-3.0, array([ 1.        ,  0.42857143, -0.71428571]))\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.eigvals(test_matrix))\n",
    "for a in gersh_circle(test_matrix):\n",
    "\tprint(shifted_eig(test_matrix, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Now that we've got (hopefully) everything that we need, we can finally bake our own home-cooked\n",
    "eigenvalue and eigenvector finder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigen_seeker(A:np.ndarray) -> Tuple[List[float], List[np.ndarray]]:\n",
    "\teigvals = []\n",
    "\teigvecs = []\n",
    "\talphas = gersh_circle(A)\n",
    "\n",
    "\tfor a in alphas:\n",
    "\t\tei_val, ei_vec = shifted_eig(A, a)\n",
    "\t\tif ei_val not in eigvals:\n",
    "\t\t\teigvals.append(ei_val)\n",
    "\t\t\teigvecs.append(ei_vec)\n",
    "\n",
    "\treturn eigvals, eigvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, -3.0],\n",
       " [array([ 1. ,  0.5, -0.5]), array([ 1.        ,  0.42857143, -0.71428571])])"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen_seeker(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy eigenvalue:\t [ 1.+0.00000000e+00j -3.+2.71947991e-15j -3.-2.71947991e-15j]\n",
      "Home-baked eigvals:\t [1.0, -3.0]\n",
      "\n",
      "Eigenvector:\t [ 1.   0.5 -0.5]\n",
      "Eigenvalue: \t 1.0\n",
      "A * Eivec:\t [ 1.   0.5 -0.5]\n",
      "Eival * Eivec:\t [ 1.   0.5 -0.5]\n",
      "\n",
      "Eigenvector:\t [ 1.          0.42857143 -0.71428571]\n",
      "Eigenvalue: \t -3.0\n",
      "A * Eivec:\t [-3.         -1.28571429  2.14285714]\n",
      "Eival * Eivec:\t [-3.         -1.28571429  2.14285714]\n"
     ]
    }
   ],
   "source": [
    "eigvals, eigvecs = eigen_seeker(test_matrix)\n",
    "\n",
    "print(\"Numpy eigenvalue:\\t\", np.linalg.eigvals(test_matrix))\n",
    "print(\"Home-baked eigvals:\\t\", eigvals)\n",
    "for va, ve in zip(eigvals, eigvecs):\n",
    "\tprint()\n",
    "\tprint(\"Eigenvector:\\t\", ve)\n",
    "\tprint(\"Eigenvalue: \\t\", va)\n",
    "\tprint(\"A * Eivec:\\t\", np.matmul(test_matrix, ve))\n",
    "\tprint(\"Eival * Eivec:\\t\", va*ve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy eigenvalue:\t [-20.61187421  -0.38812579   2.        ]\n",
      "Home-baked eigvals:\t [2.0, -0.388126, -20.611874]\n",
      "\n",
      "Eigenvector:\t [1.         0.09842976 0.25123607]\n",
      "Eigenvalue: \t 2.0\n",
      "A * Eivec:\t [2.         0.40404239 0.56585708]\n",
      "Eival * Eivec:\t [2.         0.19685951 0.50247215]\n",
      "\n",
      "Eigenvector:\t [3.24053459e-17 1.00000000e+00 3.05937104e-01]\n",
      "Eigenvalue: \t -0.388126\n",
      "A * Eivec:\t [ 6.48106917e-17 -3.88125792e-01 -1.18742081e-01]\n",
      "Eival * Eivec:\t [-1.25773573e-17 -3.88126000e-01 -1.18742144e-01]\n",
      "\n",
      "Eigenvector:\t [ 1.70586309e-20 -1.01979035e-01  1.00000000e+00]\n",
      "Eigenvalue: \t -20.611874\n",
      "A * Eivec:\t [ 3.41172618e-20  2.10197903e+00 -2.06118742e+01]\n",
      "Eival * Eivec:\t [-3.51610351e-19  2.10197901e+00 -2.06118740e+01]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "\t[2., 0, 0],\n",
    "\t[0., -1, 2],\n",
    "\t[5., 6, -20]\n",
    "])\n",
    "eigvals, eigvecs = eigen_seeker(A)\n",
    "\n",
    "print(\"Numpy eigenvalue:\\t\", np.linalg.eigvals(A))\n",
    "print(\"Home-baked eigvals:\\t\", eigvals)\n",
    "for va, ve in zip(eigvals, eigvecs):\n",
    "\tprint()\n",
    "\tprint(\"Eigenvector:\\t\", ve)\n",
    "\tprint(\"Eigenvalue: \\t\", va)\n",
    "\tprint(\"A * Eivec:\\t\", np.matmul(A, ve))\n",
    "\tprint(\"Eival * Eivec:\\t\", va*ve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "While these may still not be so good/accurate, I think it is good enough for a home-baked one.\n",
    "We'll end up using the professional one by Numpy or some other library in the future anyway.\n",
    "\n",
    "Below is some space you can try it out. Have fun and thank you for checking this project out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
